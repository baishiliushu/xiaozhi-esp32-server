import asyncio
import os
import json
import sys
import aiohttp
from typing import Optional, List, Dict, Any
from contextlib import AsyncExitStack

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client # ç¡®ä¿è¿™ä¸ªå¯¼å…¥ä¹Ÿåœ¨

from dotenv import load_dotenv # å¼•å…¥ dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

class LocalMCPClient:
    def __init__(self):
        self.exit_stack = AsyncExitStack()
        # --- vLLM Configuration ---
        # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼ˆæˆ–æŠ›å‡ºé”™è¯¯ï¼‰
        self.vllm_api_url = os.getenv("VLLM_API_URL", "http://192.168.50.205:8000/v1/chat/completions") # ä½¿ç”¨ç¯å¢ƒå˜é‡ VLLM_API_URL
        self.model_name = os.getenv("MODEL_NAME", "QW1.5BIns") # ä½¿ç”¨ç¯å¢ƒå˜é‡ MODEL_NAME

        if not self.vllm_api_url:
            raise ValueError("âŒ VLLM API URL æœªè®¾ç½®ã€‚è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® VLLM_API_URL æˆ–ç›´æ¥åœ¨ä»£ç ä¸­æä¾›é»˜è®¤å€¼ã€‚")
        if not self.model_name:
            print("âš ï¸ MODEL_NAME æœªè®¾ç½®ï¼Œå°†ä½¿ç”¨ vLLM é»˜è®¤æ¨¡å‹ï¼ˆå¦‚æœæ”¯æŒï¼‰ã€‚")
            # æˆ–è€… raise ValueError("âŒ MODEL_NAME æœªè®¾ç½®ã€‚")

        self.mcp_session: Optional[ClientSession] = None # MCP Session
        self.mcp_tools: list = [] # ä¿®æ”¹åçš„ä»£ç  (æ¨è)
        self.http_session: Optional[aiohttp.ClientSession] = None  # For async HTTP requests
        self.chat_history: List[Dict[str, Any]] = [] # å­˜å‚¨å¯¹è¯å†å²

    async def initialize_http_session(self):
        """åˆå§‹åŒ– aiohttp ä¼šè¯å¹¶æ·»åŠ åˆ°é€€å‡ºæ ˆ"""
        if not self.http_session:
            self.http_session = aiohttp.ClientSession()
            await self.exit_stack.enter_async_context(self.http_session)

    async def connect_to_mcp_server(self, server_script_path: str):
        """è¿æ¥åˆ° MCP æœåŠ¡å™¨å¹¶è·å–å·¥å…·åˆ—è¡¨"""
        if not os.path.isfile(server_script_path):
             raise FileNotFoundError(f"MCP Server è„šæœ¬æœªæ‰¾åˆ°: {server_script_path}")

        print(f"å°è¯•è¿æ¥åˆ° MCP Server è„šæœ¬: {server_script_path}...")
        is_python = server_script_path.endswith('.py')
        command = "python" if is_python else "node" # å‡è®¾æ˜¯ Python è„šæœ¬

        server_params = StdioServerParameters(
            command=command,
            args=[server_script_path],
            env=os.environ.copy() # ä¼ é€’å½“å‰ç¯å¢ƒå˜é‡ç»™å­è¿›ç¨‹
        )

        try:
             # å¯åŠ¨ MCP æœåŠ¡å™¨å¹¶å»ºç«‹é€šä¿¡
            stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
            stdio_reader, stdio_writer = stdio_transport

            # åˆ›å»º MCP å®¢æˆ·ç«¯ä¼šè¯å¹¶æ·»åŠ åˆ°é€€å‡ºæ ˆ
            self.mcp_session = ClientSession(stdio_reader, stdio_writer)
            await self.exit_stack.enter_async_context(self.mcp_session)

            await self.mcp_session.initialize()

            # åˆ—å‡ºå¹¶å­˜å‚¨ MCP æœåŠ¡å™¨ä¸Šçš„å·¥å…·
            response = await self.mcp_session.list_tools()
            self.mcp_tools = response.tools
            if not self.mcp_tools:
                 print("âš ï¸ MCP Server æœªæŠ¥å‘Šä»»ä½•å¯ç”¨å·¥å…·ã€‚")
            else:
                 print("\nâœ… å·²è¿æ¥åˆ° MCP Serverï¼Œæ”¯æŒä»¥ä¸‹å·¥å…·:", [tool.name for tool in self.mcp_tools])

        except Exception as e:
            print(f"âŒ è¿æ¥åˆ° MCP Server å¤±è´¥: {e}")
            raise # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ç¨‹åºçŸ¥é“è¿æ¥å¤±è´¥

    async def call_vllm_api(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨ vLLM çš„ OpenAI å…¼å®¹ API"""
        if not self.http_session:
            await self.initialize_http_session()

        payload = {
            "model": self.model_name,
            "messages": messages,
            "max_tokens": 1024, # å¯ä»¥è°ƒæ•´
            "temperature": 0.5, # å¯ä»¥è°ƒæ•´
        }
        # --- ç»“æŸè°ƒè¯• ---
            # --- æ·»åŠ /ç¡®ä¿è¿™è¡Œä»£ç å­˜åœ¨ ---
        headers = {"Content-Type": "application/json"}
        # --- ç»“æŸæ·»åŠ  ---

        print(f"\nğŸ”„ æ­£åœ¨å‘ vLLM å‘é€è¯·æ±‚ ({self.vllm_api_url})...")
        # print(f"Payload (éƒ¨åˆ†): model={payload['model']}, messages_count={len(payload['messages'])}, has_tools={bool(tools)}") # è°ƒè¯•ä¿¡æ¯
        try:
            async with self.http_session.post(self.vllm_api_url, headers=headers, json=payload,
                                              timeout=120) as response:
                if response.status == 200:
                    result = await response.json()
                    # --- æ·»åŠ è¿™è¡Œæ‰“å° ---
                    print(f"\nğŸ” åŸå§‹ vLLM å“åº”:\n{json.dumps(result, indent=2, ensure_ascii=False)}\n")
                    # --- ç»“æŸæ·»åŠ  ---
                    return result
                else:
                    error_text = await response.text()
                    print(f"âŒ vLLM API è¯·æ±‚å¤±è´¥ã€‚çŠ¶æ€ç : {response.status}")
                    print(f"   URL: {self.vllm_api_url}")
                    # print(f"   Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}") # å¤±è´¥æ—¶æ‰“å°å®Œæ•´Payload
                    print(f"   å“åº”å†…å®¹: {error_text[:1000]}...") # æ‰“å°éƒ¨åˆ†é”™è¯¯ä¿¡æ¯
                    return None
        except aiohttp.ClientConnectorError as e:
             print(f"âŒ è¿æ¥é”™è¯¯: æ— æ³•è¿æ¥åˆ° vLLM API at {self.vllm_api_url}: {e}")
             return None
        except asyncio.TimeoutError:
             print(f"âŒ è¶…æ—¶é”™è¯¯: è°ƒç”¨ vLLM API è¶…æ—¶ ({self.vllm_api_url})")
             return None
        except Exception as e:
            print(f"âŒ è°ƒç”¨ vLLM API æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return None

    async def process_query(self, query: str) -> Optional[str]:
        """
        å¤„ç†æŸ¥è¯¢ï¼Œå¯èƒ½è°ƒç”¨ MCP å·¥å…·ï¼Œå¹¶è¿”å›æœ€ç»ˆçš„æ–‡æœ¬å“åº”ã€‚
        """
        if not self.http_session:
            await self.initialize_http_session()  # ç¡®ä¿ http ä¼šè¯å·²åˆå§‹åŒ–

        # 1. å‡†å¤‡åˆå§‹æ¶ˆæ¯å’Œå¯ç”¨å·¥å…·åˆ—è¡¨
        # --- æ·»åŠ  System Prompt ---
        # --- ä¿®æ”¹åçš„ System Prompt ---
        system_message = {
            "role": "system",
            "content": """You are a helpful assistant. You have access to the following tools to obtain real-time information:

1.  **query_weather**:
    *   Description: æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å½“å‰å®æ—¶å¤©æ°”æƒ…å†µã€‚åŸå¸‚åç§°éœ€è¦æ˜¯è‹±æ–‡ (ä¾‹å¦‚: 'Beijing', 'London')ã€‚
    *   Arguments: `city` (string, required) - éœ€è¦æŸ¥è¯¢å¤©æ°”çš„åŸå¸‚è‹±æ–‡åç§°ã€‚

2.  **get_current_server_time**:
    *   Description: è·å–è¿è¡Œæ­¤å·¥å…·çš„æœåŠ¡å™¨ä¸Šçš„å½“å‰æ—¥æœŸå’Œæ—¶é—´ã€‚ä¸éœ€è¦ä»»ä½•è¾“å…¥å‚æ•°ã€‚
    *   Arguments: None

**é‡è¦æŒ‡ä»¤:**
å½“ä½ éœ€è¦ä½¿ç”¨å·¥å…·æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜æ—¶ï¼š
1.  åˆ¤æ–­éœ€è¦å“ªä¸ªå·¥å…·ã€‚
2.  æå–å¿…è¦çš„å‚æ•°ï¼ˆå¦‚æœæ˜¯ `query_weather`ï¼Œéœ€è¦è‹±æ–‡åŸå¸‚åï¼‰ã€‚
3.  **ä¸è¦**ç›´æ¥å›ç­”é—®é¢˜ã€‚
4.  ä½œä¸ºæ›¿ä»£ï¼Œä½ **å¿…é¡»**å›å¤ï¼Œå¹¶ä¸”**åªå›å¤**ä»¥ä¸‹æ ¼å¼çš„ JSON å¯¹è±¡ï¼Œä¸èƒ½åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€è§£é‡Šæˆ–å‰ç¼€/åç¼€ï¼š

    *   **å¦‚æœéœ€è¦è°ƒç”¨ `query_weather`:**
        ```json
        {
          "tool_name": "query_weather",
          "tool_arguments": {
            "city": "<è¿™é‡Œæ˜¯è‹±æ–‡åŸå¸‚å>"
          }
        }
        ```
        (å°† `<è¿™é‡Œæ˜¯è‹±æ–‡åŸå¸‚å>` æ›¿æ¢ä¸ºå®é™…è‹±æ–‡åŸå¸‚å)

    *   **å¦‚æœéœ€è¦è°ƒç”¨ `get_current_server_time`:**
        ```json
        {
          "tool_name": "get_current_server_time",
          "tool_arguments": {}
        }
        ```
        (å‚æ•°ä¸ºç©ºå¯¹è±¡ `{}`)

**ç¤ºä¾‹:**
- ç”¨æˆ·é—®ï¼šâ€œåŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿâ€ï¼Œä½ çš„å›ç­”**å¿…é¡»**æ˜¯ï¼š
  ```json
  {
    "tool_name": "query_weather",
    "tool_arguments": {
      "city": "Beijing"
    }
  }
  ç”¨æˆ·é—®ï¼šâ€œç°åœ¨å‡ ç‚¹äº†ï¼Ÿâ€ï¼Œä½ çš„å›ç­”å¿…é¡»æ˜¯ï¼š
  {
  "tool_name": "get_current_server_time",
  "tool_arguments": {}
  }
        å¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸éœ€è¦ä½¿ç”¨å·¥å…·ï¼ˆä¾‹å¦‚é—®å€™ã€å¸¸è¯†æ€§é—®é¢˜ç­‰ï¼‰ï¼Œè¯·åƒä¸€ä¸ªæ™®é€šçš„åŠ©æ‰‹é‚£æ ·ç›´æ¥å›ç­”ã€‚
"""
        }

        # --- ç»“æŸæ·»åŠ  ---

        current_turn_messages = []
        # ç¡®ä¿ System Prompt åœ¨æœ€å‰é¢
        if not self.chat_history or self.chat_history[0].get("role") != "system":
            current_turn_messages.append(system_message)
        elif self.chat_history and self.chat_history[0].get("role") == "system":
            # å¦‚æœå†å²è®°å½•é‡Œå·²æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œç”¨æœ€æ–°çš„æ›¿æ¢ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰
            current_turn_messages.append(system_message)  # æ€»æ˜¯ä½¿ç”¨æœ€æ–°çš„ç³»ç»Ÿæ¶ˆæ¯å®šä¹‰
            current_turn_messages.extend(self.chat_history[1:])  # æ·»åŠ é™¤æ—§ç³»ç»Ÿæ¶ˆæ¯å¤–çš„å†å²
        else:
            current_turn_messages.extend(self.chat_history)

        current_turn_messages.append({"role": "user", "content": query})

        # 2. ç¬¬ä¸€æ¬¡è°ƒç”¨ LLM (ä¸ä¼  tools)
        llm_response_data = await self.call_vllm_api(current_turn_messages)

        if not llm_response_data or not llm_response_data.get("choices"):
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ€è€ƒæ—¶é‡åˆ°äº†ä¸€äº›éº»çƒ¦ã€‚"  # ä¸ä¿®æ”¹å†å²

        choice = llm_response_data["choices"][0]
        message = choice["message"]
        llm_content = message.get("content", "").strip()

        # 3. å°†ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å“åº”ï¼ˆå¯èƒ½å«JSONï¼‰æ·»åŠ åˆ°ä¸»å†å²è®°å½•
        #    æ³¨æ„ï¼šå…ˆä¸åŠ  user messageï¼Œç­‰å¤„ç†å®Œå†ä¸€èµ·åŠ ï¼Œé¿å…å¤±è´¥æ—¶ç•™ä¸‹è®°å½•
        # self.chat_history.append({"role": "user", "content": query}) # æš‚æ—¶æ³¨é‡Š
        # self.chat_history.append(message) # æš‚æ—¶æ³¨é‡Š

        # 4. å°è¯•è§£æ JSON å·¥å…·è°ƒç”¨
        is_tool_call_parsed = False
        final_response_content = None  # ç”¨äºå­˜å‚¨æœ€ç»ˆçš„å›å¤

        try:
            pure_json_str = llm_content
            if pure_json_str.startswith("```json"):
                pure_json_str = pure_json_str[len("```json"):].strip()
            if pure_json_str.endswith("```"):
                pure_json_str = pure_json_str[:-len("```")].strip()

            tool_call_data = json.loads(pure_json_str)

            if isinstance(tool_call_data, dict) and \
                    "tool_name" in tool_call_data and \
                    "tool_arguments" in tool_call_data:

                tool_name = tool_call_data.get("tool_name")
                tool_args = tool_call_data.get("tool_arguments", {})  # é»˜è®¤ä¸ºç©ºå­—å…¸

                # æ£€æŸ¥å·¥å…·æ˜¯å¦å­˜åœ¨äº MCP Server æŠ¥å‘Šçš„åˆ—è¡¨ä¸­ (æ›´å¥å£®)
                available_tool_names = [t.name for t in self.mcp_tools]
                if tool_name in available_tool_names:
                    is_tool_call_parsed = True  # æ ‡è®°å·²è§£ææˆåŠŸ
                    print(f"\nğŸ› ï¸ è§£æåˆ°å·¥å…·è°ƒç”¨è¯·æ±‚: {tool_name} (å‚æ•°: {tool_args})")

                    # 5. è°ƒç”¨ MCP å·¥å…·
                    if self.mcp_session:
                        try:
                            print(f"   ğŸ“ æ­£åœ¨è°ƒç”¨ MCP å·¥å…· '{tool_name}'...")
                            mcp_result = await self.mcp_session.call_tool(tool_name, tool_args)
                            tool_content = mcp_result.content[0].text if mcp_result.content else "å·¥å…·æœªè¿”å›ä»»ä½•å†…å®¹ã€‚"
                            print(f"   âœ… å·¥å…·æ‰§è¡Œç»“æœ: {tool_content}")  # æ‰“å°å®Œæ•´ç»“æœ

                            # 6. å‡†å¤‡å·¥å…·ç»“æœæ¶ˆæ¯å¹¶æ·»åŠ åˆ°å†å²
                            tool_result_message_for_history = {
                                # ä½¿ç”¨ user role æ¥å‘ŠçŸ¥ LLM ç»“æœï¼Œé€šå¸¸æ›´å¯é 
                                "role": "user",
                                "content": f"å·¥å…· '{tool_name}' å·²æ‰§è¡Œï¼Œå‚æ•°ä¸º {tool_args}ã€‚ è¿”å›ç»“æœå¦‚ä¸‹ï¼š\n---\n{tool_content}\n---\nè¯·æ ¹æ®è¿™ä¸ªç»“æœï¼Œç”¨è‡ªç„¶è¯­è¨€å›ç­”æˆ‘æœ€åˆçš„é—®é¢˜ ('{query}')ã€‚"
                            }
                            # å°†ç”¨æˆ·ã€åŠ©æ‰‹(JSON)ã€å·¥å…·ç»“æœæ¶ˆæ¯ä¸€èµ·åŠ å…¥å†å²
                            self.chat_history.append({"role": "user", "content": query})
                            self.chat_history.append(message)  # åŠ©æ‰‹çš„ JSON è¾“å‡º
                            self.chat_history.append(tool_result_message_for_history)

                            # 7. ç¬¬äºŒæ¬¡è°ƒç”¨ LLM è·å–æœ€ç»ˆå›å¤
                            second_call_messages = []
                            if self.chat_history[0].get("role") != "system":
                                second_call_messages.append(system_message)  # ç¡®ä¿ System Prompt è¿˜åœ¨
                            second_call_messages.extend(self.chat_history)  # åŒ…å«åˆ°å·¥å…·ç»“æœçš„æ‰€æœ‰å†å²

                            print("ğŸ”„ æ­£åœ¨å°†å·¥å…·ç»“æœå‘é€å› LLM ä»¥ç”Ÿæˆæœ€ç»ˆå›å¤...")
                            final_response_data = await self.call_vllm_api(second_call_messages)

                            if final_response_data and final_response_data.get("choices"):
                                final_message = final_response_data["choices"][0]["message"]
                                self.chat_history.append(final_message)  # æ·»åŠ æœ€ç»ˆå›å¤åˆ°å†å²
                                final_response_content = final_message.get("content")
                            else:
                                final_response_content = "æŠ±æ­‰ï¼Œæˆ‘è·å–äº†ä¿¡æ¯ï¼Œä½†åœ¨æ€»ç»“æ—¶é‡åˆ°äº†éº»çƒ¦ã€‚"
                                # å¯ä»¥è€ƒè™‘æ˜¯å¦å°†æœ€åä¸€æ¬¡å¤±è´¥çš„åŠ©æ‰‹æ¶ˆæ¯åŠ å…¥å†å²

                        except Exception as e:
                            print(f"   âŒ æ‰§è¡Œ MCP å·¥å…· '{tool_name}' æ—¶å‡ºé”™: {e}")
                            # å°†ç”¨æˆ·ã€åŠ©æ‰‹(JSON)ã€é”™è¯¯æ¶ˆæ¯åŠ å…¥å†å²
                            self.chat_history.append({"role": "user", "content": query})
                            self.chat_history.append(message)
                            self.chat_history.append({"role": "user", "content": f"æ‰§è¡Œå·¥å…· '{tool_name}' æ—¶å‡ºé”™: {e}"})
                            final_response_content = f"æŠ±æ­‰ï¼Œåœ¨å°è¯•ä½¿ç”¨å·¥å…· '{tool_name}' æ—¶å‘ç”Ÿäº†é”™è¯¯ï¼š{e}"
                    else:
                        # MCP ä¼šè¯ä¸å­˜åœ¨
                        self.chat_history.append({"role": "user", "content": query})
                        self.chat_history.append(message)
                        self.chat_history.append({"role": "user", "content": "é”™è¯¯ï¼šæ— æ³•è°ƒç”¨å·¥å…·ï¼ŒMCPä¼šè¯æœªå»ºç«‹ã€‚"})
                        final_response_content = "é”™è¯¯ï¼šæ— æ³•è°ƒç”¨å·¥å…·ï¼ŒMCPä¼šè¯æœªå»ºç«‹ã€‚"
                else:
                    # è§£æåˆ° tool_name ä½†è¯¥å·¥å…·ä¸å¯ç”¨
                    print(f"âš ï¸ LLM è¯·æ±‚äº†ä¸å¯ç”¨çš„å·¥å…·: {tool_name}")
                    # ä»ç„¶å½“ä½œæ™®é€šæ¶ˆæ¯å¤„ç†ï¼Œæˆ–è€…ç»™å‡ºç‰¹å®šé”™è¯¯
                    pass  # ä¸‹é¢çš„ä»£ç ä¼šå¤„ç† is_tool_call_parsed = False çš„æƒ…å†µ

            else:
                # JSON æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ
                print("âš ï¸ LLM è¿”å›äº† JSONï¼Œä½†æ ¼å¼ä¸ç¬¦åˆå·¥å…·è°ƒç”¨è§„èŒƒã€‚")
                pass  # ä¸‹é¢çš„ä»£ç ä¼šå¤„ç† is_tool_call_parsed = False çš„æƒ…å†µ

        except json.JSONDecodeError:
            # å¦‚æœ llm_content ä¸æ˜¯æœ‰æ•ˆçš„ JSON
            print("â¡ï¸ LLM æœªè¿”å› JSON å·¥å…·è°ƒç”¨ï¼Œè§†ä¸ºæ™®é€šå›å¤ã€‚")
            pass  # ä¸‹é¢çš„ä»£ç ä¼šå¤„ç† is_tool_call_parsed = False çš„æƒ…å†µ
        except Exception as e:
            # å¤„ç†å…¶ä»–æ½œåœ¨é”™è¯¯
            print(f"âš ï¸ è§£æ LLM å“åº”æˆ–å¤„ç†å·¥å…·è°ƒç”¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            print(traceback.format_exc())
            # è®°å½•ç”¨æˆ·æ¶ˆæ¯å’Œé”™è¯¯
            self.chat_history.append({"role": "user", "content": query})
            # ä¸è®°å½•å¯èƒ½æœ‰é—®é¢˜çš„åŠ©æ‰‹æ¶ˆæ¯ message
            self.chat_history.append({"role": "user", "content": f"å¤„ç†å›å¤æ—¶å‘ç”Ÿé”™è¯¯: {e}"})
            final_response_content = f"å¤„ç†å›å¤æ—¶å‘ç”Ÿé”™è¯¯: {e}"

        # 8. å¦‚æœæ²¡æœ‰æˆåŠŸè§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œåˆ™å°†åŸå§‹å›å¤è§†ä¸ºæœ€ç»ˆå›å¤
        if not is_tool_call_parsed and final_response_content is None:
            # å°†ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åˆ°å†å²
            self.chat_history.append({"role": "user", "content": query})
            self.chat_history.append(message)
            final_response_content = llm_content  # ç›´æ¥ä½¿ç”¨æ¨¡å‹çš„ç¬¬ä¸€è½®å›å¤

        return final_response_content

    # ... (chat_loop å’Œ cleanup ä¸å˜) ...
    async def chat_loop(self):
        print("\nğŸ¤– æœ¬åœ° LLM + MCP å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡º")
        while True:
            try:
                query = input("\nä½ : ").strip()
                if not query:
                    continue
                if query.lower() == 'quit':
                    break
                response_text = await self.process_query(query)
                print(f"\nğŸ¤– Assistant: {response_text}")
            except KeyboardInterrupt:
                print("\næ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
                break
            except Exception as e:
                import traceback
                print(f"\nâš ï¸ å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
                print(traceback.format_exc())

    async def cleanup(self):
        print("æ­£åœ¨å…³é—­è¿æ¥...")
        await self.exit_stack.aclose()
        print("è¿æ¥å·²å…³é—­ã€‚")


async def main():
    if len(sys.argv) < 2:
        # ç°åœ¨ server_script_path æ˜¯å¿…é¡»çš„
        print("Usage: python client.py <path_to_mcp_server_script.py>")
        sys.exit(1)

    mcp_server_script = sys.argv[1]

    client = LocalMCPClient()
    try:
        # å¿…é¡»å…ˆåˆå§‹åŒ– HTTP Session
        await client.initialize_http_session()
        # è¿æ¥åˆ° MCP Server ä»¥è·å–å·¥å…·åˆ—è¡¨
        await client.connect_to_mcp_server(mcp_server_script)
        # å¼€å§‹èŠå¤©å¾ªç¯
        await client.chat_loop()
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}")
    except ValueError as e: # æ•è·é…ç½®é”™è¯¯
         print(f"é…ç½®é”™è¯¯: {e}")
    except Exception as e: # æ•è·å…¶ä»–å¯åŠ¨æ—¶é”™è¯¯
         print(f"å¯åŠ¨å®¢æˆ·ç«¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
         import traceback
         print(traceback.format_exc())
    finally:
        await client.cleanup()


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹: python client.py server.py
    asyncio.run(main())